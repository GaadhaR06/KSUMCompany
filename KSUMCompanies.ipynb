{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8470c8af-26b5-48e7-bd51-1ec1f4dd6f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e7b51fd6-71fe-4419-9449-b9f8f544ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "6aa52a55-efee-4fdf-9d6b-ff7506f0016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://startups.startupmission.in/startups/8qjEZ'\n",
    "# response = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a5218c13-14be-4373-9103-fcf9acff7973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(response.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "66070ee4-6df2-4094-898b-026890518ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elements = soup.find_all('div', class_=\"flex gap-1 items-start\")\n",
    "# product_names = soup.find_all('div', class_ = 'text-xl font-bold mb-3')\n",
    "# product = product_names[1].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "63ff7565-0fcd-4451-b31b-10d72e29c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(elements[0].text.strip() if len(elements) > 0 else \"Not Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "47f3c97f-8363-48ef-91aa-d2cd91481e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(elements[3].text.strip() if len(elements) > 0 else \"Not Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7bbf319a-f741-446f-9654-f1cc97d40be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(elements)):\n",
    "#     print(elements[i].text.strip() if len(elements) > 0 else \"Not Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "62570c32-d2b2-4e72-a31f-e4a5a7ed1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sector = \" \".join(elements[0].text.split()) if len(elements) > 0 else \"Not Found\"\n",
    "\n",
    "# industry = \" \".join(elements[1].text.split()) if len(elements) > 0 else \"Not Found\"\n",
    "\n",
    "\n",
    "# bizmodel = \" \".join(elements[2].text.split()) if len(elements) > 0 else \"Not Found\"\n",
    "\n",
    "# tech = \" \".join(elements[3].text.split()) if len(elements) > 0 else \"Not Found\"\n",
    "\n",
    "# print(sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "cf1a7b48-7273-4ee6-b06d-ff681f86973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     'product': [product],\n",
    "#     'sector': [sector],\n",
    "#     'Industry': [industry],\n",
    "#     'Business Models': [bizmodel],\n",
    "#     'Technology': [tech]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1f5e5c49-d57f-4a6e-82be-ed73f98f480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(data)\n",
    "# df\n",
    "# # Print DataFrame\n",
    "# # print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "ff699b71-c3d9-4c66-b3d6-2062e01c2fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs for Ernakulam or Thiruvananthapuram:\n",
      "Processing: https://startups.startupmission.in/startups/ZdX4Z\n",
      "Processing: https://startups.startupmission.in/startups/p6rgE\n",
      "Processing: https://startups.startupmission.in/startups/poXQV\n",
      "Processing: https://startups.startupmission.in/startups/poE3Z\n",
      "Processing: https://startups.startupmission.in/startups/8Xoyp\n",
      "Processing: https://startups.startupmission.in/startups/ZDVEp\n",
      "Processing: https://startups.startupmission.in/startups/0JgYp\n",
      "Processing: https://startups.startupmission.in/startups/8qjEZ\n",
      "Processing: https://startups.startupmission.in/startups/07mQA\n",
      "Processing: https://startups.startupmission.in/startups/ZEw1p\n",
      "Processing: https://startups.startupmission.in/startups/0wgm1\n",
      "Processing: https://startups.startupmission.in/startups/pQynE\n",
      "Processing: https://startups.startupmission.in/startups/Z1M5Z\n",
      "Processing: https://startups.startupmission.in/startups/pvVql\n",
      "Processing: https://startups.startupmission.in/startups/8VDAv\n",
      "Processing: https://startups.startupmission.in/startups/Z2370\n",
      "Processing: https://startups.startupmission.in/startups/ZD57K\n",
      "Processing: https://startups.startupmission.in/startups/8L5om\n",
      "Processing: https://startups.startupmission.in/startups/Z9QVP\n",
      "Processing: https://startups.startupmission.in/startups/8RAPg\n",
      "Processing: https://startups.startupmission.in/startups/ZjGbG\n",
      "Processing: https://startups.startupmission.in/startups/8AG1x\n",
      "Processing: https://startups.startupmission.in/startups/07mbM\n",
      "Processing: https://startups.startupmission.in/startups/pQlq0\n",
      "Processing: https://startups.startupmission.in/startups/8Vg5Z\n",
      "Processing: https://startups.startupmission.in/startups/py658\n",
      "Processing: https://startups.startupmission.in/startups/8XkLY\n",
      "Processing: https://startups.startupmission.in/startups/ZD7x2\n",
      "Processing: https://startups.startupmission.in/startups/Z3eAx\n",
      "Processing: https://startups.startupmission.in/startups/pzgXQ\n",
      "Processing: https://startups.startupmission.in/startups/pkkJm\n",
      "Processing: https://startups.startupmission.in/startups/Z3ezJ\n",
      "Processing: https://startups.startupmission.in/startups/p69E0\n",
      "Processing: https://startups.startupmission.in/startups/8XkRD\n",
      "Processing: https://startups.startupmission.in/startups/0Brd0\n",
      "Processing: https://startups.startupmission.in/startups/Z1ERZ\n",
      "Processing: https://startups.startupmission.in/startups/8n1eD\n",
      "Processing: https://startups.startupmission.in/startups/pQXX0\n",
      "Processing: https://startups.startupmission.in/startups/8XaDZ\n",
      "Processing: https://startups.startupmission.in/startups/ZNrRZ\n",
      "Processing: https://startups.startupmission.in/startups/Z9dQ8\n",
      "Processing: https://startups.startupmission.in/startups/8LWxp\n",
      "Processing: https://startups.startupmission.in/startups/04Nn8\n",
      "Processing: https://startups.startupmission.in/startups/ZaPn8\n",
      "Processing: https://startups.startupmission.in/startups/ZEebk\n",
      "Processing: https://startups.startupmission.in/startups/Z9WMP\n",
      "Processing: https://startups.startupmission.in/startups/8AvlZ\n",
      "Processing: https://startups.startupmission.in/startups/pkVr0\n",
      "Processing: https://startups.startupmission.in/startups/04q2p\n",
      "Processing: https://startups.startupmission.in/startups/8L6r0\n",
      "Processing: https://startups.startupmission.in/startups/NZOgZ\n",
      "Processing: https://startups.startupmission.in/startups/04Yap\n",
      "Processing: https://startups.startupmission.in/startups/poa3Z\n",
      "Processing: https://startups.startupmission.in/startups/8xOBb\n",
      "Processing: https://startups.startupmission.in/startups/ZEk1p\n",
      "Processed 55 companies successfully.\n",
      "Data saved to companies_data_check2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_company_data(url):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        Dictionary containing company information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Sending GET request to the URL\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Initialize company data dictionary\n",
    "        company_data = {\n",
    "            'company_name': '',\n",
    "            'website': '',\n",
    "            'email': '',\n",
    "            'linkedin': '',\n",
    "            'incorporation_date': '',\n",
    "            'incorporation_type': '',\n",
    "            'registered_address': '',\n",
    "            'office_address': '',\n",
    "            'sectors': [],\n",
    "            'business_models': [],\n",
    "            'industries': [],\n",
    "            'technologies': [],\n",
    "            'team': [],\n",
    "            'products': []\n",
    "        }\n",
    "        \n",
    "        # Extract company name\n",
    "        company_name_elem = soup.select_one('.md\\\\:mt-10.mt-2.font-bold.text-xl.text-blue-700.mb-5')\n",
    "        if company_name_elem:\n",
    "            company_data['company_name'] = company_name_elem.text.strip()\n",
    "        \n",
    "        # Extract website URL\n",
    "        website_link = soup.select_one('a[href^=\"http\"]:has(svg.icon-tabler-world-www)')\n",
    "        if website_link:\n",
    "            company_data['website'] = website_link.get('href')\n",
    "        \n",
    "        # Extract LinkedIn URL\n",
    "        linkedin_link = soup.select_one('a[href*=\"linkedin.com\"]:has(svg.icon-tabler-brand-linkedin)')\n",
    "        if linkedin_link:\n",
    "            company_data['linkedin'] = linkedin_link.get('href')\n",
    "        \n",
    "        # Extract basic company information\n",
    "        info_sections = soup.select('.bg-white.shadow-md.rounded-lg.mt-5.px-5.py-5.space-y-3')\n",
    "        for section in info_sections:\n",
    "            # Extract incorporation information\n",
    "            incorporation_date = section.select_one('div:-soup-contains(\"Incorporation Date\") + div')\n",
    "            if incorporation_date:\n",
    "                company_data['incorporation_date'] = incorporation_date.text.strip()\n",
    "            \n",
    "            incorporation_type = section.select_one('div:-soup-contains(\"Incorporation Type\") + div')\n",
    "            if incorporation_type:\n",
    "                company_data['incorporation_type'] = incorporation_type.text.strip()\n",
    "            \n",
    "            # Extract address information\n",
    "            registered_address = section.select_one('div:-soup-contains(\"Registered Address\") + div')\n",
    "            if registered_address:\n",
    "                company_data['registered_address'] = registered_address.text.strip()\n",
    "            \n",
    "            office_address = section.select_one('div:-soup-contains(\"Office Address\") + div')\n",
    "            if office_address:\n",
    "                company_data['office_address'] = office_address.text.strip()\n",
    "            \n",
    "            # Extract team information\n",
    "            team_members = section.select('.flex.flex-col.items-center.justify-start.text-center')\n",
    "            for member in team_members:\n",
    "                name = member.select_one('.font-bold')\n",
    "                role = name.find_next_sibling() if name else None\n",
    "                \n",
    "                if name and role:\n",
    "                    company_data['team'].append({\n",
    "                        'name': name.text.strip(),\n",
    "                        'role': role.text.strip()\n",
    "                    })\n",
    "        \n",
    "        # Extracting sectors, business models, industries, technologies\n",
    "        info_grid = soup.select_one('.grid.md\\\\:grid-cols-2.gap-2')\n",
    "        if info_grid:\n",
    "            # Extract sectors\n",
    "            sectors = info_grid.select_one('.flex.gap-1.items-start:-soup-contains(\"Sector:\") .flex.flex-wrap.gap-1.items-start span')\n",
    "            if sectors:\n",
    "                company_data['sectors'] = [sector.text.strip() for sector in info_grid.select('.flex.gap-1.items-start:-soup-contains(\"Sector:\") .flex.flex-wrap.gap-1.items-start span')]\n",
    "            \n",
    "            # Extract business models\n",
    "            business_models = info_grid.select_one('.flex.gap-1.items-start:-soup-contains(\"Business Model:\") .flex.flex-wrap.gap-1.items-start span')\n",
    "            if business_models:\n",
    "                company_data['business_models'] = [model.text.strip() for model in info_grid.select('.flex.gap-1.items-start:-soup-contains(\"Business Model:\") .flex.flex-wrap.gap-1.items-start span')]\n",
    "            \n",
    "            # Extract industries\n",
    "            industries = info_grid.select_one('.flex.gap-1.items-start:-soup-contains(\"Industry:\") .flex.flex-wrap.gap-1.items-start span')\n",
    "            if industries:\n",
    "                company_data['industries'] = [industry.text.strip() for industry in info_grid.select('.flex.gap-1.items-start:-soup-contains(\"Industry:\") .flex.flex-wrap.gap-1.items-start span')]\n",
    "            \n",
    "            # Extract technologies\n",
    "            technologies = info_grid.select_one('.flex.gap-1.items-start:-soup-contains(\"Technology:\") .flex.flex-wrap.gap-1.items-start span')\n",
    "            if technologies:\n",
    "                company_data['technologies'] = [tech.text.strip() for tech in info_grid.select('.flex.gap-1.items-start:-soup-contains(\"Technology:\") .flex.flex-wrap.gap-1.items-start span')]\n",
    "        \n",
    "        # Extract product information\n",
    "        product_sections = soup.select('#products .bg-white.shadow-md.rounded-lg.mt-5.px-5.py-5')\n",
    "        for product in product_sections:\n",
    "            product_name = product.select_one('.text-xl.font-bold.mb-3')\n",
    "            product_desc = product.select_one('.text-justify')\n",
    "            \n",
    "            if product_name:\n",
    "                product_data = {\n",
    "                    'name': product_name.text.strip(),\n",
    "                    'description': product_desc.text.strip() if product_desc else '',\n",
    "                    'sectors': [],\n",
    "                    'industries': [],\n",
    "                    'business_models': [],\n",
    "                    'technologies': []\n",
    "                }\n",
    "                \n",
    "                # Extract product-specific sectors\n",
    "                product_sectors = product.select('.flex.gap-1.items-start:-soup-contains(\"Sector:\") .flex.flex-wrap.gap-1.items-start span')\n",
    "                if product_sectors:\n",
    "                    product_data['sectors'] = [sector.text.strip() for sector in product_sectors]\n",
    "                \n",
    "                # Extract product-specific industries\n",
    "                product_industries = product.select('.flex.gap-1.items-start:-soup-contains(\"Industry:\") .flex.flex-wrap.gap-1.items-start span')\n",
    "                if product_industries:\n",
    "                    product_data['industries'] = [industry.text.strip() for industry in product_industries]\n",
    "                \n",
    "                # Extract product-specific business models\n",
    "                product_models = product.select('.flex.gap-1.items-start:-soup-contains(\"Business Models:\") .flex.flex-wrap.gap-1.items-start span')\n",
    "                if product_models:\n",
    "                    product_data['business_models'] = [model.text.strip() for model in product_models]\n",
    "                \n",
    "                # Extract product-specific technologies\n",
    "                product_techs = product.select('.flex.gap-1.items-start:-soup-contains(\"Technology:\") .flex.flex-wrap.gap-1.items-start span')\n",
    "                if product_techs:\n",
    "                    product_data['technologies'] = [tech.text.strip() for tech in product_techs]\n",
    "                \n",
    "                company_data['products'].append(product_data)\n",
    "        \n",
    "        return company_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data: {e}\")\n",
    "        return None\n",
    "\n",
    "# def save_to_json(company_data, output_file='company_data.json'):\n",
    "#     \"\"\"Save extracted company data to a JSON file\"\"\"\n",
    "#     try:\n",
    "#         with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#             json.dump(company_data, f, indent=4, ensure_ascii=False)\n",
    "#         print(f\"Data saved to {output_file}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving data: {e}\")\n",
    "\n",
    "# def print_company_summary(company_data):\n",
    "#     \"\"\"Print a summary of the company data\"\"\"\n",
    "#     if not company_data:\n",
    "#         print(\"No data available\")\n",
    "#         return\n",
    "\n",
    "# For console output\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*50)\n",
    "    # print(f\"COMPANY: {company_data['company_name']}\")\n",
    "    # print(\"=\"*50)\n",
    "    \n",
    "    # print(f\"\\nWebsite: {company_data['website']}\")\n",
    "    # print(f\"LinkedIn: {company_data['linkedin']}\")\n",
    "    # print(f\"Incorporation: {company_data['incorporation_date']} ({company_data['incorporation_type']})\")\n",
    "    \n",
    "    # print(\"\\nSECTORS:\")\n",
    "    # for sector in company_data['sectors']:\n",
    "    #     print(f\"- {sector}\")\n",
    "    \n",
    "    # print(\"\\nBUSINESS MODELS:\")\n",
    "    # for model in company_data['business_models']:\n",
    "    #     print(f\"- {model}\")\n",
    "    \n",
    "    # print(\"\\nINDUSTRIES:\")\n",
    "    # for industry in company_data['industries']:\n",
    "    #     print(f\"- {industry}\")\n",
    "    \n",
    "    # print(\"\\nTECHNOLOGIES:\")\n",
    "    # for tech in company_data['technologies']:\n",
    "    #     print(f\"- {tech}\")\n",
    "    \n",
    "    # print(\"\\nADDRESSES:\")\n",
    "    # print(f\"Registered: {company_data['registered_address']}\")\n",
    "    # print(f\"Office: {company_data['office_address']}\")\n",
    "    \n",
    "    # print(\"\\nTEAM MEMBERS:\")\n",
    "    # for member in company_data['team']:\n",
    "    #     print(f\"- {member['name']} ({member['role']})\")\n",
    "    \n",
    "    # print(\"\\nPRODUCTS:\")\n",
    "    # for i, product in enumerate(company_data['products'], 1):\n",
    "    #     print(f\"\\n{i}. {product['name']}\")\n",
    "    #     print(f\"   Description: {product['description']}\")\n",
    "        \n",
    "    #     if product['sectors']:\n",
    "    #         print(\"   Sectors: \" + \", \".join(product['sectors']))\n",
    "        \n",
    "    #     if product['industries']:\n",
    "    #         print(\"   Industries: \" + \", \".join(product['industries']))\n",
    "        \n",
    "    #     if product['business_models']:\n",
    "    #         print(\"   Business Models: \" + \", \".join(product['business_models']))\n",
    "        \n",
    "    #     if product['technologies']:\n",
    "    #         print(\"   Technologies: \" + \", \".join(product['technologies']))\n",
    "# Main Function\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of URLs to scrape \n",
    "    print(\"URLs for Ernakulam or Thiruvananthapuram:\")\n",
    "    # leng = 0  \n",
    "    urls_to_scrape = []\n",
    "    locationDetails = []\n",
    "    inUrls = [\n",
    "        'https://startups.startupmission.in/startups?sector=Banking%2C+Financial+Services+and+Insurance&industry=FinTech',\n",
    "        'https://startups.startupmission.in/startups?sector=Banking%2C+Financial+Services+and+Insurance&industry=FinTech&page=2',\n",
    "        'https://startups.startupmission.in/startups?sector=Banking%2C+Financial+Services+and+Insurance&industry=FinTech&page=3',\n",
    "        'https://startups.startupmission.in/startups?sector=Banking%2C+Financial+Services+and+Insurance&industry=FinTech&page=4',\n",
    "        'https://startups.startupmission.in/startups?sector=Banking%2C+Financial+Services+and+Insurance&industry=FinTech&page=5'\n",
    "    ]\n",
    "\n",
    "    # Get the page\n",
    "    for inUrl in inUrls:\n",
    "        response = requests.get(inUrl)\n",
    "        html_code = response.text\n",
    "\n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "        # Initialize a list to store the URLs\n",
    "        urls = []\n",
    "\n",
    "    # Find all startup cards (the <a> elements)\n",
    "        startup_cards = soup.select('.container .grid a')\n",
    "\n",
    "    # Alternative if the above doesn't work (the HTML might be incomplete)\n",
    "        if not startup_cards:\n",
    "            startup_cards = soup.find_all('a')\n",
    "\n",
    "# Loop through each card\n",
    "        for card in startup_cards:\n",
    "        # Find the location text\n",
    "            location_element = card.select_one('.text-sm')\n",
    "            if location_element:\n",
    "                location_text = location_element.text.strip().lower()\n",
    "            # Check if the location contains \"ernakulam\" or \"thiruvananthapuram\"\n",
    "                if \"ernakulam\" in location_text or \"thiruvananthapuram\" in location_text:\n",
    "                    # Add the URL to our list\n",
    "                    urls.append(card['href'])\n",
    "                    locationDetails.append(location_text)\n",
    "\n",
    "# Print the results\n",
    "        for url in urls:\n",
    "            urls_to_scrape.append(url)\n",
    "        # leng = leng + len(urls)\n",
    "    # print(urls_to_scrape)\n",
    "    # print(leng)\n",
    "    # print(len(locationDetails))\n",
    "# These are individually given urls in version 1\n",
    "#     urls_to_scrape = [\n",
    "#         # \"https://startups.startupmission.in/startups/pQlq0\",\n",
    "#         # \"https://startups.startupmission.in/startups/8Vg5Z\",\n",
    "#         # \"https://startups.startupmission.in/startups/py658\",\n",
    "#         # \"https://startups.startupmission.in/startups/8XkLY\",\n",
    "#         # \"https://startups.startupmission.in/startups/ZD7x2\",\n",
    "#         # \"https://startups.startupmission.in/startups/Z3eAx\",\n",
    "#         # \"https://startups.startupmission.in/startups/pzgXQ\",\n",
    "#         # \"https://startups.startupmission.in/startups/pkkJm\",\n",
    "#         # \"https://startups.startupmission.in/startups/Z3ezJ\",\n",
    "#         # \"https://startups.startupmission.in/startups/p69E0\",\n",
    "#         # \"https://startups.startupmission.in/startups/8XkRD\",\n",
    "#         # \"https://startups.startupmission.in/startups/0Brd0\",\n",
    "#         # \"https://startups.startupmission.in/startups/Z1ERZ\",\n",
    "#         # \"https://startups.startupmission.in/startups/8n1eD\",\n",
    "#         # \"https://startups.startupmission.in/startups/pQXX0\",\n",
    "#         # \"https://startups.startupmission.in/startups/8XaDZ\",\n",
    "#         # \"https://startups.startupmission.in/startups/ZNrRZ\",\n",
    "#         # \"https://startups.startupmission.in/startups/Z9dQ8\",\n",
    "#         # \"https://startups.startupmission.in/startups/8LWxp\",\n",
    "#         # \"https://startups.startupmission.in/startups/04Nn8\",\n",
    "#         # \"https://startups.startupmission.in/startups/ZaPn8\",\n",
    "#         # \"https://startups.startupmission.in/startups/ZEebk\",\n",
    "#         # \"https://startups.startupmission.in/startups/Z9WMP\",\n",
    "#         # \"https://startups.startupmission.in/startups/8AvlZ\",\n",
    "#         # \"https://startups.startupmission.in/startups/pkVr0\",\n",
    "#         # \"https://startups.startupmission.in/startups/04q2p\",\n",
    "#         # \"https://startups.startupmission.in/startups/8L6r0\",\n",
    "#         # \"https://startups.startupmission.in/startups/NZOgZ\",\n",
    "#         # \"https://startups.startupmission.in/startups/04Yap\",\n",
    "#         # \"https://startups.startupmission.in/startups/poa3Z\",\n",
    "#         # \"https://startups.startupmission.in/startups/8xOBb\",\n",
    "#         # \"https://startups.startupmission.in/startups/ZEk1p\"\n",
    "#         'https://startups.startupmission.in/startups/8nJa8'\n",
    "#     ]\n",
    "    \n",
    "    # Processing each URL\n",
    "    all_companies_data = []\n",
    "    \n",
    "    for url in urls_to_scrape:\n",
    "        print(f\"Processing: {url}\")\n",
    "        \n",
    "        # Extracting data\n",
    "        company_data = extract_company_data(url)\n",
    "        \n",
    "        # Printing summary\n",
    "        # print_company_summary(company_data)\n",
    "        \n",
    "        # Save individual JSON file to check\n",
    "        # json_filename = f\"{company_data['company_name'].replace(' ', '_').lower()}_data.json\"\n",
    "        # save_to_json(company_data, json_filename)\n",
    "        \n",
    "        # Add to the list\n",
    "        all_companies_data.append(company_data)\n",
    "    \n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    df = pd.DataFrame(all_companies_data)\n",
    "    \n",
    "    # Processing columns containing lists or dictionaries\n",
    "    for col in df.columns:\n",
    "        if len(df) > 0 and df[col].notna().any():\n",
    "            sample_value = df[col].iloc[0]\n",
    "            \n",
    "            # Handle list of strings\n",
    "            if isinstance(sample_value, list) and all(isinstance(item, str) for item in sample_value):\n",
    "                df[col] = df[col].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "            \n",
    "            # Handle list of dictionaries - convert to JSON string\n",
    "            elif isinstance(sample_value, list) and any(isinstance(item, dict) for item in sample_value):\n",
    "                df[col] = df[col].apply(lambda x: json.dumps(x) if isinstance(x, list) else x)\n",
    "    \n",
    "    # Create a separate DataFrame for products\n",
    "    product_rows = []\n",
    "    for company in all_companies_data:\n",
    "        company_name = company.get('company_name', '')\n",
    "        products = company.get('products', [])\n",
    "        \n",
    "        if products:\n",
    "            for product in products:\n",
    "                if isinstance(product, dict):\n",
    "                    product_dict = {\n",
    "                        'company_name': company_name,\n",
    "                        'product_name': product.get('name', ''),\n",
    "                        'product_description': product.get('description', '')\n",
    "                    }\n",
    "                    \n",
    "                    # Handle nested lists within products\n",
    "                    for key in ['sectors', 'industries', 'business_models', 'technologies']:\n",
    "                        if key in product:\n",
    "                            values = product.get(key, [])\n",
    "                            product_dict[f'product_{key}'] = ', '.join(values) if isinstance(values, list) else values\n",
    "                    \n",
    "                    product_rows.append(product_dict)\n",
    "    \n",
    "    products_df = pd.DataFrame(product_rows) if product_rows else None\n",
    "    df['Location'] = locationDetails\n",
    "    \n",
    "    # Save to Excel file with multiple sheets\n",
    "    excel_filename = \"companies_data_check2.xlsx\"\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='Companies', index=False)\n",
    "        \n",
    "        if products_df is not None and not products_df.empty:\n",
    "            products_df.to_excel(writer, sheet_name='Products', index=False)\n",
    "    \n",
    "    print(f\"Processed {len(all_companies_data)} companies successfully.\")\n",
    "    print(f\"Data saved to {excel_filename}\")\n",
    "    # print(len(locationDetails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e8c57-488b-4d86-9d73-d55e439cf0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "\n",
    "# print(\"URLs for Ernakulam or Thiruvananthapuram:\")\n",
    "# # leng = 0  \n",
    "# urls_to_scrape = []\n",
    "# inUrls = [\n",
    "#     'https://startups.startupmission.in/startups?sector=Banking%2C+Financial+Services+and+Insurance&industry=FinTech',\n",
    "#     'https://startups.startupmission.in/startups?sector=Banking%2C+Financial+Services+and+Insurance&industry=FinTech&page=2',\n",
    "#     'https://startups.startupmission.in/startups?sector=Banking%2C+Financial+Services+and+Insurance&industry=FinTech&page=3',\n",
    "#     'https://startups.startupmission.in/startups?sector=Banking%2C+Financial+Services+and+Insurance&industry=FinTech&page=4',\n",
    "#     'https://startups.startupmission.in/startups?sector=Banking%2C+Financial+Services+and+Insurance&industry=FinTech&page=5'\n",
    "# ]\n",
    "\n",
    "# # Get the page\n",
    "# for inUrl in inUrls:\n",
    "#     response = requests.get(inUrl)\n",
    "#     html_code = response.text\n",
    "\n",
    "#     # Parse the HTML\n",
    "#     soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "#     # Initialize a list to store the URLs\n",
    "#     urls = []\n",
    "\n",
    "# # Find all startup cards (the <a> elements)\n",
    "#     startup_cards = soup.select('.container .grid a')\n",
    "\n",
    "#     # Alternative if the above doesn't work (the HTML might be incomplete)\n",
    "#     if not startup_cards:\n",
    "#         startup_cards = soup.find_all('a')\n",
    "\n",
    "# # Loop through each card\n",
    "#     for card in startup_cards:\n",
    "#         # Find the location text\n",
    "#         location_element = card.select_one('.text-sm')\n",
    "#         if location_element:\n",
    "#             location_text = location_element.text.strip().lower()\n",
    "        \n",
    "#         # Check if the location contains \"ernakulam\" or \"thiruvananthapuram\"\n",
    "#             if \"ernakulam\" in location_text or \"thiruvananthapuram\" in location_text:\n",
    "#                 # Add the URL to our list\n",
    "#                 urls.append(card['href'])\n",
    "\n",
    "# # Print the results\n",
    "      \n",
    "#     for url in urls:\n",
    "#         urls_to_scrape.append(url)\n",
    "#     leng = leng + len(urls)\n",
    "# urls_to_scrape\n",
    "# # print(leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30597c61-7edb-4d64-9944-dc09af2dec1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ENV_NLP]",
   "language": "python",
   "name": "conda-env-ENV_NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
